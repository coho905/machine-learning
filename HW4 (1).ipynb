{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multinomial Naive Bayesian\n",
        "\n"
      ],
      "metadata": {
        "id": "Z5D_AEkGbbHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# load data, select features, train test split\n",
        "data = pd.read_csv(\"hw4_naive.csv\")\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "DHJMlOmEJEQi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training multinomical naive bayes classifer. this function goes in and finds probability of being in a class and the probability of features showing up in a class. smoothing set to one by default\n",
        "def train_naive(X, y, alpha=1.0):\n",
        "  n_samples, n_features = X.shape\n",
        "  # find unique classes\n",
        "  classes = np.unique(y)\n",
        "  n_classes = len(classes)\n",
        "  class_priors = np.zeros(n_classes)\n",
        "  # probability of being a class in the dataset\n",
        "  for i, c in enumerate(classes):\n",
        "    class_priors[i] = np.sum(y == c) / n_samples\n",
        "   # find unique features\n",
        "  feature_values = []\n",
        "  for j in range(n_features):\n",
        "    values = np.unique(X[:, j])\n",
        "    feature_values.append(values)\n",
        "  feature_probs = {}\n",
        "  for i, c in enumerate(classes):\n",
        "    X_c = X[y == c]\n",
        "    count_c = X_c.shape[0]\n",
        "    for j in range(n_features):\n",
        "      for val in feature_values[j]:\n",
        "        count = np.sum(X_c[:, j] == val)\n",
        "        # with smoothing, P(feature|class)\n",
        "        prob = (count + alpha) / (count_c + alpha * len(feature_values[j]))\n",
        "        feature_probs[(i, j, val)] = prob\n",
        "  return {'classes': classes,'class_priors': class_priors,'feature_probs': feature_probs}"
      ],
      "metadata": {
        "id": "SCGhTXNEbGTy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes in model, uses it to run inference on input vector X\n",
        "def predict_naive(model, X):\n",
        "  # load in from model\n",
        "  classes = model['classes']\n",
        "  class_priors = model['class_priors']\n",
        "  feature_probs = model['feature_probs']\n",
        "\n",
        "  n_samples = X.shape[0]\n",
        "  n_classes = len(classes)\n",
        "  predictions = np.zeros(n_samples, dtype=int)\n",
        "  # use log probability to not go into numerical underflow\n",
        "  for i in range(n_samples):\n",
        "    log_probs = np.zeros(n_classes)\n",
        "    for j in range(n_classes):\n",
        "      log_prob = np.log(class_priors[j])\n",
        "      for k in range(X.shape[1]):\n",
        "        feature_val = X[i, k]\n",
        "        # feature probability in a given class\n",
        "        if (j, k, feature_val) in feature_probs:\n",
        "          prob = feature_probs[(j, k, feature_val)]\n",
        "        else:\n",
        "          # infinitesmial small\n",
        "          prob = 1e-10\n",
        "        log_prob += np.log(prob)\n",
        "      log_probs[j] = log_prob\n",
        "    # return class with highest probability\n",
        "    predictions[i] = classes[np.argmax(log_probs)]\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "ejfSYYg_JkSF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_naive(X_train, y_train, alpha=1.0) #train on data\n",
        "y_pred = predict_naive(model, X_test) #test on testing"
      ],
      "metadata": {
        "id": "kzCwS4PJbRre"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred) #calculate accuracy\n",
        "f1 = f1_score(y_test, y_pred) #calculate f1\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcFCv21ibYKB",
        "outputId": "56e7f93e-1c29-4aa9-f8cb-b898e756276b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8232\n",
            "F1 Score: 0.7585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Naive Bayes"
      ],
      "metadata": {
        "id": "A_n4XZGhuzVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train a gaussian naive bayes classifier\n",
        "def gaussian_train(X, y):\n",
        "  # find unique classes\n",
        "  classes = np.unique(y)\n",
        "  class_count = {}\n",
        "  mean = {}\n",
        "  var = {}\n",
        "  for c in classes:\n",
        "    # data points where we are in class c\n",
        "    X_c = X[y == c]\n",
        "    # number of data points\n",
        "    class_count[c] = X_c.shape[0]\n",
        "    # mean of features for a class\n",
        "    mean[c] = np.mean(X_c, axis=0)\n",
        "    # variance of features for a class\n",
        "    var[c] = np.var(X_c, axis=0)\n",
        "  return classes, class_count, mean, var"
      ],
      "metadata": {
        "id": "WSHLMtMCu1kn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Calculate Gaussian probability density function\n",
        "def calculate_likelihood(x, c, mean, var):\n",
        "  mean_c = mean[c]\n",
        "  var_c = var[c]\n",
        "  #to prevent 0s\n",
        "  epsilon = 1e-10\n",
        "  var_c = var_c + epsilon\n",
        "  likelihood = 1\n",
        "  for i in range(len(x)):\n",
        "    # pdf on var and mean\n",
        "    exponent = np.exp(-((x[i] - mean_c[i])**2) / (2 * var_c[i]))\n",
        "    likelihood *= (1 / np.sqrt(2 * np.pi * var_c[i])) * exponent\n",
        "    #return likelihood of observing features x given class c\n",
        "  return likelihood"
      ],
      "metadata": {
        "id": "9hDzU3il_roI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the class of a single sample x\n",
        "def predict_one(x, classes, class_count, mean, var):\n",
        "  posts = []\n",
        "  for c in classes:\n",
        "    # calculute probs for each class\n",
        "    prior = class_count[c] / sum(class_count.values())\n",
        "    likelihood = calculate_likelihood(x, c, mean, var)\n",
        "    posterior = prior * likelihood\n",
        "    posts.append(posterior)\n",
        "  # return the highest prob\n",
        "  return classes[np.argmax(posts)]"
      ],
      "metadata": {
        "id": "pYf9aSiO_hRO"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference, predicting the output of all samples\n",
        "def gaussian_predict(X, classes, class_count, mean, var):\n",
        "   predictions = []\n",
        "   for x in X:\n",
        "       predictions.append(predict_one(x, classes, class_count, mean, var))\n",
        "   return np.array(predictions)"
      ],
      "metadata": {
        "id": "UYedC1K__TBq"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training and inferencing\n",
        "classes, class_count, mean, var = gaussian_train(X_train, y_train)\n",
        "y_pred = gaussian_predict(X_test, classes, class_count, mean, var)"
      ],
      "metadata": {
        "id": "cXK1czpr_bwP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy and f1\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2BPVnwF_-wo",
        "outputId": "2bed6d88-f59d-4de1-89d4-af3d8f82f472"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5973\n",
            "F1 Score: 0.3135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison\n",
        "\n",
        "Multinomial naive bayes seems to be significantly better than Gaussian naive bayes, likely due to the discrete nature of the data points"
      ],
      "metadata": {
        "id": "zgweiLUI2apa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-means\n",
        "\n"
      ],
      "metadata": {
        "id": "nNC0dbMtuLMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "aZgMtJnXbZWI"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# euclidian distance\n",
        "def distance(point1, point2):\n",
        "  return np.sqrt(np.sum((point1 - point2) ** 2))"
      ],
      "metadata": {
        "id": "CwA1VWXoynKL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate centroid of a cluster of points, only uses mean\n",
        "def calc_centroid(cluster_points, method='mean'):\n",
        "  if len(cluster_points) == 0:\n",
        "    return None\n",
        "  # return the mean of each position\n",
        "  return np.mean(cluster_points, axis=0)"
      ],
      "metadata": {
        "id": "4-THr8oOu-lJ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intitalize the centroids with 2 different methods\n",
        "def initalize_centroids(data, k, initilization='random seed'):\n",
        "  if initilization == 'random seed':\n",
        "    # take random indeces\n",
        "    index = random.sample(range(len(data)), k)\n",
        "    return data[index]\n",
        "  elif initilization == 'random split':\n",
        "    # assign each one point to a random centroid\n",
        "    cluster_assignments = np.random.randint(0, k, size=len(data))\n",
        "    centroids = np.zeros((k, data.shape[1]))\n",
        "    for i in range(k):\n",
        "      cluster_points = data[cluster_assignments == i]\n",
        "      centroids[i] = calc_centroid(cluster_points, 'mean')\n",
        "    return centroids\n",
        "  return None"
      ],
      "metadata": {
        "id": "GTiknBUlxZYW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_means_clustering(data, k, max_iter=100, method='mean', initilization='random seed'):\n",
        "  #initalize clusters\n",
        "  centroids = initalize_centroids(data, k, initilization)\n",
        "  iterations = 0\n",
        "  # last centroid for each point\n",
        "  prev_assignments = None\n",
        "  current_assignments = np.zeros(data.shape[0], dtype=int)\n",
        "  # measures if previous assignments are diff as current\n",
        "  changed = True\n",
        "  # not converged and more iterations to go\n",
        "  while iterations < max_iter and changed:\n",
        "    distances = np.zeros(k)\n",
        "    # for each data point\n",
        "    for i in range(data.shape[0]):\n",
        "      # for each centroid\n",
        "      for j in range(k):\n",
        "        # calculate distance from current point to each centroid\n",
        "        distances[j] = distance(data[i], centroids[j])\n",
        "      # assign to the closest centroid\n",
        "      current_assignments[i] = np.argmin(distances)\n",
        "    # converged\n",
        "    if prev_assignments is not None and np.array_equal(prev_assignments, current_assignments):\n",
        "      changed = False\n",
        "    # change previous assignments as we prepare for another loop\n",
        "    prev_assignments = current_assignments.copy()\n",
        "    # calculate the new mean of the k centroids\n",
        "    for j in range(k):\n",
        "      cluster_points = data[current_assignments == j]\n",
        "      centroids[j] = calc_centroid(cluster_points, method)\n",
        "    # increase the iterations by 1 as we continues\n",
        "    iterations += 1\n",
        "  # assigning final clusters and the center position of each cluster\n",
        "  clusters = []\n",
        "  # for each center, add the points assigned there to the appropriate cluster\n",
        "  for i in range(k):\n",
        "    mask = current_assignments == i\n",
        "    clusters.append(data[mask])\n",
        "  # return the clusters of points and the centroids themselves\n",
        "  return clusters, centroids"
      ],
      "metadata": {
        "id": "2F3dgEGOyCT-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run it on the given data\n",
        "clusters = pd.read_csv('hw4_cluster.csv')\n",
        "clusters.head()\n",
        "clusters= clusters.values"
      ],
      "metadata": {
        "id": "IVYBxe8Gzqix"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find clusters and centroids on data\n",
        "final_clusters, centroids = k_means_clustering(clusters, k=5, max_iter=50, method='mean', initilization='random seed')"
      ],
      "metadata": {
        "id": "E2UBn0Uk1Md1"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate silohouette scors\n",
        "def silhouette_score(clusters):\n",
        "  # Combine all clusters into one big array and create mapping\n",
        "  all_points = np.vstack(clusters) if len(clusters) > 0 else np.array([])\n",
        "  n = all_points.shape[0]\n",
        "  point_to_cluster = {}\n",
        "  point_index = 0\n",
        "  for cluster_idx, cluster in enumerate(clusters):\n",
        "    for _ in range(len(cluster)):\n",
        "      point_to_cluster[point_index] = cluster_idx\n",
        "      point_index += 1\n",
        "\n",
        "  silhouette_values = []\n",
        "  for i in range(n):\n",
        "    point = all_points[i]\n",
        "    cluster_idx = point_to_cluster[i]\n",
        "    # Calculate average distance to points in the same cluster\n",
        "    a_i = 0\n",
        "    same_cluster_points = 0\n",
        "    for j in range(n):\n",
        "      if i != j and point_to_cluster[j] == cluster_idx:\n",
        "        a_i += distance(point, all_points[j])\n",
        "        same_cluster_points += 1\n",
        "\n",
        "    if same_cluster_points > 0:\n",
        "      a_i /= same_cluster_points\n",
        "    else:\n",
        "      a_i = 0\n",
        "    # Calculate minimum avg distance to points in diff clusters\n",
        "    b_i = float('inf')\n",
        "\n",
        "    for other_cluster_idx in range(len(clusters)):\n",
        "      if other_cluster_idx != cluster_idx:\n",
        "        avg_distance = 0\n",
        "        other_cluster_points = 0\n",
        "        for j in range(n):\n",
        "          if point_to_cluster[j] == other_cluster_idx:\n",
        "            avg_distance += distance(point, all_points[j])\n",
        "            other_cluster_points += 1\n",
        "        if other_cluster_points > 0:\n",
        "          avg_distance /= other_cluster_points\n",
        "          b_i = min(b_i, avg_distance)\n",
        "\n",
        "    if b_i == float('inf'):\n",
        "      b_i = 0\n",
        "\n",
        "    if a_i == 0 and b_i == 0:\n",
        "      silhouette_i = 0\n",
        "    else:\n",
        "      silhouette_i = (b_i - a_i) / max(a_i, b_i)\n",
        "\n",
        "    silhouette_values.append(silhouette_i)\n",
        "  # Return the average silhouette value across all points\n",
        "  return np.mean(silhouette_values)\n",
        "\n",
        "score = silhouette_score(final_clusters)\n",
        "print(f\"Silhouette score for k=5: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSNZW-qB1f5s",
        "outputId": "86b1ec30-2d79-43b9-e80c-659a91a38b72"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette score for k=5: 0.5901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus"
      ],
      "metadata": {
        "id": "aX_wPaMlAR6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "highest = -2 #outside of silhouette range\n",
        "k=-1\n",
        "#loop through all ks asked for\n",
        "for i in range(2, 6):\n",
        "  final_clusters, centroids = k_means_clustering(clusters, k=i, max_iter=50, method='mean', initilization='random seed')\n",
        "  score = silhouette_score(final_clusters)\n",
        "  if score > highest:\n",
        "    highest = score\n",
        "    k = i\n",
        "  print(f\"Silhouette score for k={i}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n\\n\\n\\nHighest Silhouette score is: {highest:.4f} when k={k}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw3cJHRu3NEw",
        "outputId": "68e6e42f-2a93-4769-984f-06d531d8e4b9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette score for k=2: 0.7689\n",
            "Silhouette score for k=3: 0.7192\n",
            "Silhouette score for k=4: 0.6294\n",
            "Silhouette score for k=5: 0.5901\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Highest Silhouette score is: 0.7689 when k=2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K=2 appears to be the best as it has the highest Silhouette score of any k in range from k=2 to k=5"
      ],
      "metadata": {
        "id": "rZ0r2PmY8zzI"
      }
    }
  ]
}